{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Homework\n",
    "\n",
    "> Note: sometimes your answer doesn't match one of the options exactly. \n",
    "> That's fine. \n",
    "> Select the option that's closest to your solution."
   ],
   "id": "3dd3e30636cdf03c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "In this homework, we will use the Bank Marketing dataset. Download it from [here](https://archive.ics.uci.edu/static/public/222/bank+marketing.zip).\n",
    "\n",
    "Or you can do it with `wget`:\n",
    "\n",
    "```bash\n",
    "wget https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\n",
    "```\n",
    "\n",
    "We need to take `bank/bank-full.csv` file from the downloaded zip-file.\n",
    "In this dataset our desired target for classification task will be `y` variable - has the client subscribed a term deposit or not."
   ],
   "id": "2daccd79c8bc504e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Features\n",
    "\n",
    "For the rest of the homework, you'll need to use only these columns:\n",
    "\n",
    "* `age`,\n",
    "* `job`,\n",
    "* `marital`,\n",
    "* `education`,\n",
    "* `balance`,\n",
    "* `housing`,\n",
    "* `contact`,\n",
    "* `day`,\n",
    "* `month`,\n",
    "* `duration`,\n",
    "* `campaign`,\n",
    "* `pdays`,\n",
    "* `previous`,\n",
    "* `poutcome`,\n",
    "* `y`"
   ],
   "id": "363e7d4613e696b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "\n",
    "* Select only the features from above.\n",
    "* Check if the missing values are presented in the features."
   ],
   "id": "c46de826e1bd64fa"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:32.487366Z",
     "start_time": "2024-10-12T13:55:32.368798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:32.543728Z",
     "start_time": "2024-10-12T13:55:32.491043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"bank-full.csv\", sep=\";\")\n",
    "features = [\"age\", \"job\", \"marital\", \"education\", \"balance\", \"housing\", \"contact\",\n",
    "            \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"poutcome\"]\n",
    "target = \"y\"\n",
    "df = df[features + [target]]\n",
    "\n",
    "numeric = []\n",
    "categorical = []\n",
    "for f in features:\n",
    "    if df.dtypes[f] == \"object\":\n",
    "        categorical.append(f)\n",
    "    else:\n",
    "        numeric.append(f)"
   ],
   "id": "d5579bbc0a292bf7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:32.792132Z",
     "start_time": "2024-10-12T13:55:32.779189Z"
    }
   },
   "cell_type": "code",
   "source": "df.isnull().sum()",
   "id": "a49e32778a2d1964",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          0\n",
       "job          0\n",
       "marital      0\n",
       "education    0\n",
       "balance      0\n",
       "housing      0\n",
       "contact      0\n",
       "day          0\n",
       "month        0\n",
       "duration     0\n",
       "campaign     0\n",
       "pdays        0\n",
       "previous     0\n",
       "poutcome     0\n",
       "y            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 1\n",
    "\n",
    "What is the most frequent observation (mode) for the column `education`?"
   ],
   "id": "55c62ce26bb792e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:32.878073Z",
     "start_time": "2024-10-12T13:55:32.874195Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"The most frequent observation (mode) for the column education is {df['education'].mode().iloc[0]}\")",
   "id": "4dd50cc38dba4069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent observation (mode) for the column education is secondary\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 2\n",
    "\n",
    "Create the [correlation matrix](https://www.google.com/search?q=correlation+matrix) for the numerical features of your dataset. \n",
    "In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?"
   ],
   "id": "c82abc96eaaa404d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:32.955501Z",
     "start_time": "2024-10-12T13:55:32.944804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "np.fill_diagonal(corr.values, np.nan)\n",
    "\n",
    "corr_unstacked = corr.unstack()\n",
    "max_corr_pair = corr_unstacked.idxmax()\n",
    "print(f\"The two features that have the biggest correlation are {max_corr_pair[0]} and {max_corr_pair[1]}\")"
   ],
   "id": "ae4e9f6bec70b18c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two features that have the biggest correlation are pdays and previous\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Target encoding\n",
    "\n",
    "* Now we want to encode the `y` variable.\n",
    "* Let's replace the values `yes`/`no` with `1`/`0`."
   ],
   "id": "e6917c8eb432798a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:33.078823Z",
     "start_time": "2024-10-12T13:55:33.068080Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"y\"] = df[\"y\"].replace({\"yes\": 1, \"no\": 0}).astype(int)",
   "id": "12a0d80012206bae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12206/561725355.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"y\"] = df[\"y\"].replace({\"yes\": 1, \"no\": 0}).astype(int)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split the data\n",
    "\n",
    "* Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "    * Use Scikit-Learn for that (the `train_test_split` function) and set the seed to `42`.\n",
    "* Make sure that the target value `y` is not in your dataframe."
   ],
   "id": "4dcd0610c20d5302"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:33.156593Z",
     "start_time": "2024-10-12T13:55:33.137319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "y_train = df_train[target]\n",
    "y_val = df_val[target]\n",
    "y_test = df_test[target]"
   ],
   "id": "20cb2e37aee97496",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "\n",
    "* Calculate the mutual information score between `y` and other categorical variables in the dataset. Use the training set only.\n",
    "* Round the scores to 2 decimals using `round(score, 2)`.\n",
    "\n",
    "Which of these variables has the biggest mutual information score?"
   ],
   "id": "a02ad9a7f6430ca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:33.338498Z",
     "start_time": "2024-10-12T13:55:33.208463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mutual_scores = []\n",
    "for c in categorical:\n",
    "    mutual_score = mutual_info_score(y_train, df_train[c])\n",
    "    mutual_scores.append(mutual_score)\n",
    "\n",
    "print(f\"{categorical[np.argmax(mutual_scores)]} has the biggest mutual information score\")"
   ],
   "id": "a81f82a6102a50f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poutcome has the biggest mutual information score\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "\n",
    "* Now let's train a logistic regression.\n",
    "* Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "* Fit the model on the training dataset.\n",
    "    - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "    - `model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)`\n",
    "* Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "What accuracy did you get?"
   ],
   "id": "3e754e25b0d6f12d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:33.758569Z",
     "start_time": "2024-10-12T13:55:33.356638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "train_dict = df_train[features].to_dict(orient=\"records\")\n",
    "val_dict = df_val[features].to_dict(orient=\"records\")\n",
    "test_dict = df_test[features].to_dict(orient=\"records\")\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "X_test = dv.transform(test_dict)"
   ],
   "id": "beb4afa848f31c04",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:33.968784Z",
     "start_time": "2024-10-12T13:55:33.777961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, model.predict(X_val))\n",
    "print(f\"The accuracy of the model on the validation dataset is {round(acc, 2)}\")"
   ],
   "id": "a59fbd03fd25ae46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the validation dataset is 0.9\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 5 \n",
    "\n",
    "* Let's find the least useful feature using the *feature elimination* technique.\n",
    "* Train a model with all these features (using the same parameters as in Q4).\n",
    "* Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "* For each feature, calculate the difference between the original accuracy and the accuracy without the feature. \n",
    "\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "> **Note**: The difference doesn't have to be positive."
   ],
   "id": "df107c78cbc3166e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:40.464284Z",
     "start_time": "2024-10-12T13:55:33.987305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_acc = acc\n",
    "differences = []\n",
    "\n",
    "for f in features:\n",
    "    reduced_features = features.copy()\n",
    "    reduced_features.remove(f)\n",
    "    reduced_train_dict = df_train[reduced_features].to_dict(orient=\"records\")\n",
    "    reduced_val_dict = df_val[reduced_features].to_dict(orient=\"records\")\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_reduced_train = dv.fit_transform(reduced_train_dict)\n",
    "    X_reduced_val = dv.transform(reduced_val_dict)\n",
    "    model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "    model.fit(X_reduced_train, y_train)\n",
    "    current_acc = accuracy_score(y_val, model.predict(X_reduced_val))\n",
    "    difference = original_acc - current_acc\n",
    "    differences.append(difference)\n",
    "\n",
    "print(f\"{features[np.argmin(differences)]} has the smallest difference\")"
   ],
   "id": "df155fe7c62ff930",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age has the smallest difference\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 6\n",
    "\n",
    "* Now let's train a regularized logistic regression.\n",
    "* Let's try the following values of the parameter `C`: `[0.01, 0.1, 1, 10, 100]`.\n",
    "* Train models using all the features as in Q4.\n",
    "* Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "\n",
    "Which of these `C` leads to the best accuracy on the validation set?\n",
    "\n",
    "> **Note**: If there are multiple options, select the smallest `C`."
   ],
   "id": "cae1bee8445e997d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:55:41.493663Z",
     "start_time": "2024-10-12T13:55:40.493130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "accs = []\n",
    "for C in alphas:\n",
    "    model = LogisticRegression(solver=\"liblinear\", C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    accs.append(round(acc, 3))\n",
    "    \n",
    "print(f\"C={alphas[np.argmax(accs)]} leads to the best accuracy on the validation set\")"
   ],
   "id": "5e065256a4505b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.1 leads to the best accuracy on the validation set\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2024/homework/hw03\n",
    "* If your answer doesn't match options exactly, select the closest one"
   ],
   "id": "7709e125a9ca573c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
